import whisper
import sounddevice as sd
import numpy as np
import queue
import time
import tempfile
import scipy.io.wavfile as wav

# Whisper ëª¨ë¸ (GPU ìˆìœ¼ë©´ large-v2, CPUë©´ small/medium ê¶Œì¥)
model = whisper.load_model("small")

SAMPLE_RATE = 16000
CHANNELS = 1
BLOCK_SECONDS = 0.5       # ì˜¤ë””ì˜¤ ë¸”ë¡ í¬ê¸° (0.5ì´ˆì”© ì½ê¸°)
CHUNK_SECONDS = 3.0       # ì´ë§Œí¼ ëª¨ì´ë©´ í•œ ë²ˆ ì¸ì‹ (ì§€ì—°/ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„)
VAD_THRESHOLD = 500       # ë¬´ìŒ ê°ì§€ ì„ê³„ê°’ (í™˜ê²½ì— ë§ì¶° ì¡°ì •)
USE_VAD = True            # ê°„ë‹¨ VAD ì‚¬ìš© í† ê¸€

audio_q = queue.Queue()

def audio_callback(indata, frames, time_info, status):
    if status:
        print(f"âš ï¸ Audio status: {status}")
    # int16 â†’ íì— ë„£ê¸°
    audio_q.put(indata.copy())

def is_speech(chunk_int16, threshold=VAD_THRESHOLD):
    # ì ˆëŒ€ ì—ë„ˆì§€ë¡œ ê°„ë‹¨í•œ VAD
    return np.mean(np.abs(chunk_int16)) > threshold

print("ğŸ¤ ì‹¤ì‹œê°„ Whisper ì‹œì‘ (ì¢…ë£Œ: Ctrl+C)")
buffer = []

try:
    with sd.InputStream(samplerate=SAMPLE_RATE,
                        channels=CHANNELS,
                        dtype='int16',
                        blocksize=int(SAMPLE_RATE * BLOCK_SECONDS),
                        callback=audio_callback):
        last_transcribe = time.time()
        while True:
            # íì—ì„œ ë¸”ë¡ ê°€ì ¸ì™€ ë²„í¼ì— ì¶•ì 
            while not audio_q.empty():
                block = audio_q.get()
                buffer.append(block)

            # ëˆ„ì  ê¸¸ì´ í™•ì¸
            total_samples = sum(b.shape[0] for b in buffer)
            if total_samples >= int(SAMPLE_RATE * CHUNK_SECONDS):
                # ë²„í¼ ë³‘í•©
                audio_chunk = np.concatenate(buffer, axis=0).flatten()
                buffer.clear()

                # VADë¡œ ë¬´ìŒì´ë©´ ìŠ¤í‚µ
                if USE_VAD and not is_speech(audio_chunk):
                    # ë¬´ìŒì´ë©´ ë„ˆë¬´ ì˜¤ë˜ ìŠ¤í‚µí•˜ì§€ ì•Šë„ë¡ ì†ŒëŸ‰ë§Œ ìœ ì§€
                    continue

                # int16 â†’ float32 ì •ê·œí™”
                audio_f32 = audio_chunk.astype(np.float32) / 32768.0
                # ë ˆë²¨ ë…¸ë©€ë¼ì´ì¦ˆ
                max_amp = np.max(np.abs(audio_f32)) + 1e-8
                audio_f32 = audio_f32 / max_amp

                # WhisperëŠ” íŒŒì¼ ê²½ë¡œ ì…ë ¥ì´ í¸í•˜ë‹ˆ ì„ì‹œ WAVë¡œ ì €ì¥ í›„ í˜¸ì¶œ
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                    wav.write(f.name, SAMPLE_RATE, (audio_f32 * 32767).astype(np.int16))
                    result = model.transcribe(
                        f.name,
                        language="ko",
                        task="transcribe",
                        fp16=False,
                        temperature=0.0,
                        beam_size=5,
                        best_of=5,
                        no_speech_threshold=0.2,
                        logprob_threshold=-1.0
                    )
                text = result.get("text", "").strip()
                if text:
                    print(f"ğŸ—£ï¸ {text}")
                else:
                    print("ğŸ”‡ (ë¬´ìŒ ë˜ëŠ” ì €ì‹ ë¢° ë°œí™”)")

            # ë„ˆë¬´ ë°”ì˜ì§€ ì•Šê²Œ ì•½ê°„ ì‰¼
            time.sleep(0.01)

except KeyboardInterrupt:
    print("\nğŸ›‘ ì‹¤ì‹œê°„ ì¸ì‹ ì¢…ë£Œ")